@Proceedings{ISIPTA2019,
   booktitle = {Proceedings of the Eleventh International Symposium on Imprecise Probabilities: Theories and Applications},
   name = {International Symposium on Imprecise Probabilities: Theories and Applications},
   shortname = {ISIPTA},
   sections = {Preface|Full Papers|Short Papers},
   year = {2019},
   editor = {De Bock, Jasper and {de Campos}, Cassio P. and {de Cooman}, Gert and Quaeghebeur, Erik and Wheeler, Gregory},
   volume = {103},
   start = {2019-07-03},
   end = {2019-07-06},
   published = {2019-06-29},
   address = {Thagaste, Ghent, Belgium},
   url = {htttp://sipta.org/isipta2019},
   conference_number = {11}
}

@InProceedings{debock19a,
	title = {ISIPTA 2019: Preface},
	author = {De Bock, Jasper and {de Campos}, Cassio P. and {de Cooman}, Gert and Quaeghebeur, Erik and Wheeler, Gregory},
	pages = {1},
	abstract = {The ISIPTA meetings are the primary forum for presenting and discussing advances in imprecise probabilities research. They are organized once every two years by SIPTA, the Society for Imprecise Probabilities: Theories and Applications. The first meeting was held in Ghent in 1999. It was followed by meetings in Ithaca, Lugano, Pittsburgh, Prague, Durham, Innsbruck, Compiègne, Pescara and Lugano. After twenty years, we return to Ghent for the <em>11th International Symposium on Imprecise Probabilities: Theories and Applications</em>. This anniversary edition will be held from Wednesday 3 to Saturday 6 July 2019.},
	keywords = {ISIPTA 2019, preface},
	section = {Preface}
}

@InProceedings{abrams19,
	title = {Natural Selection with Objective Imprecise Probability},
	author = {Abrams, Marshall},
	pages = {2--13},
	abstract = {I argue that natural selection sometimes depends on objective imprecise probabilities. I give a general argument for the existence of objective imprecise probabilities. I then argue that natural selection, whether involving objective imprecise probabilities or not, would give rise to organisms whose behavior was imprecisely probabilistic, and that this would mean that other organisms’ environments were imprecisely probabilistic. Since natural selection can be influenced by the environment, it therefore sometimes depends on objective imprecise probability. I explain why the absence of reports of objective imprecise probability in evolution is nevertheless unsurprising, and provide illustrations of ways to model natural selection with objective imprecise probabilities.},
	keywords = {objective imprecise probability, biological fitness, natural selection, set-chain, hi-lo method},
	section = {Full Papers}
}

@InProceedings{antonucci19,
	title = {Credal Sentential Decision Diagrams},
	author = {Antonucci, Alessandro and Facchini, Alessandro and Mattei, Lilith},
	pages = {14--22},
	abstract = {<em>Probabilistic sentential decision diagrams</em> are logical circuits annotated by probability mass functions on the disjunctive gates. This allows for a compact representation of joint mass functions consistent with logical constraints. We propose a <em>credal</em> generalisation of the probabilistic quantification of these models, that allows to replace the local probabilities with (credal) sets of mass functions specified by linear constraints. This induces a joint credal set, that sharply assigns probability zero to states inconsistent with the constraints. These models can support cautious estimates of the local parameters when only small amounts of training data are available. Algorithmic strategies to compute lower and upper bounds of marginal and conditional queries are provided. The task can be achieved in linear time with respect to the diagram size for marginal queries. The same can be done for conditional queries if the topology of the circuit is singly connected.},
	keywords = {probabilistic graphical models, credal sets, logical constraints, arithmetic circuits, sentential decision diagrams, sum-product networks},
	section = {Full Papers}
}

@InProceedings{benavoli19,
	title = {Bernstein’s Socks, Polynomial-Time Provable Coherence and Entanglement},
	author = {Benavoli, Alessio and Facchini, Alessandro and Zaffalon, Marco},
	pages = {23--31},
	abstract = {We recently introduced a bounded rationality approach for the theory of desirable gambles. It is based on the unique requirement that being nonnegative for a gamble has to be defined so that it can be provable in polynomial time. In this paper we continue to investigate properties of this class of models. In particular we verify that the space of Bernstein polynomials in which nonnegativity is specified by the Krivine-Vasilescu certificate is yet another instance of this theory. As a consequence, we show how it is possible to construct in it a thought experiment uncovering entanglement with classical (hence non quantum) coins.},
	keywords = {theory of desirable gambles, abstract logic, bounded rationality, Bernstein polynomials, entanglement},
	section = {Full Papers}
}

@InProceedings{bolt19,
	title = {On Minimum Elementary-Triplet Bases for Independence Relations},
	author = {Bolt, Janneke and {van der Gaag}, Linda C.},
	pages = {32--37},
	abstract = {A semi-graphoid independence relation is a set of independence statements, called triplets, and is typically exponentially large in the number of variables involved. For concise representation of such a relation, a subset of its triplets is listed in a so-called basis; its other triplets are defined implicitly through a set of axioms. An elementary-triplet basis for this purpose consists of all elementary triplets of a relation. Such a basis however, may include redundant information. In this paper we provide two lower bounds on the size of an elementary-triplet basis in general and an upper bound on the size of a minimum elementary-triplet basis. We further specify the construction of an elementary-triplet basis of minimum size for restricted relations.},
	keywords = {Independence relations, Axioms of independence, Elementary triplets, Basis representation},
	section = {Full Papers}
}

@InProceedings{bradley19,
	title = {Aggregating Belief Models},
	author = {Bradley, Seamus},
	pages = {38--48},
	abstract = {This paper has two goals. The first goal is to say something about how one might combine different agents’ imprecise probabilities to generate an aggregate imprecise probability. The second goal is to champion the very general theory of “belief models” (de Cooman “Belief models: an order theoretic investigation” Annals of Mathematics and AI 2005) which, I think, deserves more attention. The belief models framework is interesting partly because many other formal models of reasoning appear as special cases of belief models (for example, propositional logic, ranking functions, imprecise probability).},
	keywords = {Belief Models, Aggregation, Lower Previsions, Lattice Theory},
	section = {Full Papers}
}

@InProceedings{cahoon19,
	title = {Possibility Measures for Valid Statistical Inference Based on Censored Data},
	author = {Cahoon, Joyce and Martin, Ryan},
	pages = {49--58},
	abstract = {Inferential challenges that arise when data are corrupted by censoring have been extensively studied under the classical frameworks. In this paper, we provide an alternative approach based on a generalized inferential model whose output is a data-dependent possibility distribution. This construction is driven by an association between the censored data, parameter of interest, and unobserved auxiliary variable that takes the form of a relative likelihood. The possibility distribution then emerges from the introduction of a nested random set designed to predict that unobserved auxiliary variable and is calibrated to achieve certain frequentist guarantees. The performance of the proposed method is investigated using real and simulated data.},
	keywords = {Inferential model, Kaplan--Meier estimator, likelihood, Monte Carlo, random set},
	section = {Full Papers}
}

@InProceedings{carranzaalarcon19,
	title = {Imprecise Gaussian Discriminant Classification},
	author = {Carranza Alarcon, Yonatan Carlos and Destercke, S\'{e}bastien},
	pages = {59--67},
	abstract = {Gaussian discriminant analysis is a popular classification model, that in the precise case can produce unreliable predictions in case of high uncertainty. While imprecise probability theory offer a nice theoretical framework to solve this issue, it has not been yet applied to Gaussian discriminant analysis. This work remedies this, by proposing a new Gaussian discriminant analysis based on robust Bayesian analysis and near-ignorance priors. The model delivers <em>cautious</em> predictions, in form of set-valued class, in case of limited or imperfect available information. Experiments show that including an imprecise component in the Gaussian discriminant analysis produces reasonably cautious predictions, in the sense that the number of set-valued predictions is not too high, and that those predictions correspond to hard-to-classify instances, that is instances for which the precise classifier accuracy drops.},
	keywords = {Discriminant Analysis, Robust Bayesian, Classification, Near-ignorance},
	section = {Full Papers}
}

@InProceedings{cella19,
	title = {Incorporating Expert Opinion in an Inferential Model while Maintaining Validity},
	author = {Cella, Leonardo and Martin, Ryan},
	pages = {68--77},
	abstract = {The incorporation of partial prior information in statistical inference problems still lacks a definitive answer. The two most popular statistical schools of thought deal with partial priors in different ways: they either get completely ignored (frequentist approach) or they are transformed into a “complete” prior information, i.e., a probability distribution (Bayesian approach). Acknowledging the importance of (i) taking into account all sources of relevant information in a given problem and (ii) controlling error probabilities, the present paper provides insights on how to incorporate partial priors “as they are”. This incorporation is guided by desired properties, such as that correct partial priors should result in more efficient inferences and, most importantly, that the inferences are always calibrated, independent of the truthfulness of the partial prior.},
	keywords = {Dempster's rule, elastic, plausibility contour, prior distribution, random set},
	section = {Full Papers}
}

@InProceedings{correia19,
	title = {An Experimental Study of Prior Dependence in Bayesian Network Structure Learning},
	author = {Correia, Alvaro Henrique Chaim and {de Campos}, Cassio P. and {van der Gaag}, Linda C.},
	pages = {78--81},
	abstract = {The Bayesian Dirichlet equivalent uniform (BDeu) function is a popular score to evaluate the goodness of a Bayesian network structure given complete categorical data. Despite its interesting properties, such as likelihood equivalence, it does require a prior expressed via a user-defined parameter known as Equivalent Sample Size (ESS), which significantly affects the final structure. We study conditions to obtain prior independence in BDeu-based structure learning. We show in experiments that the amount of data needed to render the learning robust to different ESS values is prohibitively large, even in big data times.},
	keywords = {Robustness, Bayesian Networks, Structure Learning, BDeu},
	section = {Short Papers}
}

@InProceedings{corsato19,
	title = {Extending Nearly-Linear Models},
	author = {Corsato, Chiara and Pelessoni, Renato and Vicig, Paolo},
	pages = {82--90},
	abstract = {Nearly-Linear Models are a family of neighbourhood models, obtaining lower/upper probabilities from a given probability by a linear affine transformation with barriers. They include a number of known models as special cases, among them the Pari-Mutuel Model, the $\varepsilon$-contamination model, the Total Variation Model and the vacuous lower/upper probabilities. We classified Nearly-Linear models, investigating their consistency properties, in previous work. Here we focus on how to extend those Nearly-Linear Models that are coherent or at least avoid sure loss. We derive formulae for their natural extensions, interpret a specific model as a natural extension itself of a certain class of lower probabilities, and supply a risk measurement interpretation for one of the natural extensions we compute.},
	keywords = {Pari-Mutuel Model, Nearly-Linear Models, natural extension, coherent lower probabilities, risk measures, Value at Risk, Expected Shortfall},
	section = {Full Papers}
}

@InProceedings{cozman19a,
	title = {The Joy of Probabilistic Answer Set Programming},
	author = {Cozman, Fabio},
	pages = {91--101},
	abstract = {Probabilistic answer set programming (PASP) combines rules, facts, and independent probabilistic facts. Often one restricts such programs so that every query yields a sharp probability value. The purpose of this paper is to argue that a very useful modeling language is obtained by adopting a particular credal semantics for PASP, where one associates with each consistent program a credal set. We examine the basic properties of PASP and present an algorithm to compute (upper) probabilities given a program.},
	keywords = {Logic programming, Answer set programming, Probabilistic programming, Credal sets},
	section = {Full Papers}
}

@InProceedings{cozman19b,
	title = {Graphoid Properties of Variants of Epistemic Independence Based on Regular Conditioning},
	author = {Cozman, Fabio},
	pages = {102--110},
	abstract = {Graphoid properties attempt to capture the most important features of abstract “independence”. We examine which semi-graphoid properties are satisfied by various concepts of independence for credal sets; we focus on variants of epistemic, confirmational, and type-5 independence that are based on regular conditioning.},
	keywords = {Graphoid properties, Credal sets, Epistemic irrelevance and independence},
	section = {Full Papers}
}

@InProceedings{crane19,
	title = {Imprecise Probabilities as a Semantics for Intuitive Probabilistic Reasoning},
	author = {Crane, Harry},
	pages = {111--120},
	abstract = {I prove a connection between the logical framework for intuitive probabilistic reasoning (IPR) introduced by Crane (2017) and sets of probabilities. More specifically, this connection provides a straightforward interpretation of imprecise probabilities as subjective credal states, giving a formal semantics for Crane’s IPR proposal.},
	keywords = {imprecise probability, credence, credal state, intuitionistic logic, Martin-Löf type theory, evidence, subjective belief, justified belief},
	section = {Full Papers}
}

@InProceedings{danielson19,
	title = {Decideit 3.0: Software for Second-Order Based Decision Evaluations},
	author = {Danielson, Mats and Ekenberg, Love and Larsson, Aron},
	pages = {121--124},
	abstract = {In this paper, we discuss representation and evaluation in the DecideIT 3.0 decision tool which is based on a belief mass interpretation of the background information. The decision components are imprecise in terms of intervals and qualitative estimates and we emphasise how multiplicative and additive aggregations influence the resulting belief distribution over the expected values.},
	keywords = {Belief distribution, decision analysis, bounded Dirichlet, skew-normal distribution},
	section = {Short Papers}
}

@InProceedings{debock19b,
	title = {Interpreting, Axiomatising and Representing Coherent Choice Functions in Terms of Desirability},
	author = {De Bock, Jasper and {de Cooman}, Gert},
	pages = {125--134},
	abstract = {Choice functions constitute a simple, direct and very general mathematical framework for modelling choice under uncertainty. In particular, they are able to represent the set-valued choices that appear in imprecise-probabilistic decision making. We provide these choice functions with a clear interpretation in terms of desirability, use this interpretation to derive a set of basic coherence axioms, and show that this notion of coherence leads to a representation in terms of sets of strict preference orders. By imposing additional properties such as totality, the mixing property and Archimedeanity, we obtain representation in terms of sets of strict total orders, lexicographic probability systems, coherent lower previsions or linear previsions.},
	keywords = {choice functions, coherence, desirability, representation, non-binary choice models},
	section = {Full Papers}
}

@InProceedings{decadt19,
	title = {Monte Carlo Estimation for Imprecise Probabilities: Basic Properties},
	author = {Decadt, Arne and {de Cooman}, Gert and De Bock, Jasper},
	pages = {135--144},
	abstract = {We describe Monte Carlo methods for estimating lower envelopes of expectations of real random variables. We prove that the estimation bias is negative and that its absolute value shrinks with increasing sample size. We discuss fairly practical techniques for proving strong consistency of the estimators and use these to prove the consistency of an example in the literature. We also provide an example where there is no consistency.},
	keywords = {Monte Carlo simulation, Imprecise probabilities, Bias, Consistency, Lower expectation operator, Estimation},
	section = {Full Papers}
}

@InProceedings{denoeux19,
	title = {An Axiomatic Utility Theory for Dempster-Shafer Belief Functions},
	author = {Denoeux, Thierry and Shenoy, Prakash P.},
	pages = {145--155},
	abstract = {The main goal of this paper is to describe an axiomatic utility theory for Dempster-Shafer belief function lotteries. The axiomatic framework used is analogous to von Neumann-Morgenstern’s utility theory for probabilistic lotteries as described by Luce and Raiffa. Unlike the probabilistic case, our axiomatic framework leads to interval-valued utilities, and therefore, to a partial (incomplete) preference order on the set of all belief function lotteries. If the belief function reference lotteries we use are Bayesian belief functions, then our representation theorem coincides with Jaffray’s representation theorem for his linear utility theory for belief functions. We illustrate our framework using some examples discussed in the literature. Finally, we compare our decision theory with those proposed by Jaffray and Smets.},
	keywords = {Dempster-Shafer theory of evidence, von Neumann-Morgenstern's utility theory, Jaffray's linear utility theory, Smets' decision theory},
	section = {Full Papers}
}

@InProceedings{dewit19,
	title = {Robustness in Sum-Product Networks with Continuous and Categorical Data},
	author = {{de Wit}, Rob and {de Campos}, Cassio P. and Conaty, Diarmaid and {del Rincon}, Jesus Martinez},
	pages = {156--158},
	abstract = {Sum-product networks are a popular family of probabilistic graphical models for which marginal inference can be performed in polynomial time. After learning sum-product networks from scarce data, small variations of parameters could lead to different conclusions. We adapt the robustness measure created for categorical credal sum-product networks to domains with both continuous and categorical variables. We apply this approach to a real-world dataset of online purchases where the goal is to identify fraudulent cases. We empirically show that such credal models can better discriminate between easy and hard instances than simply using the probability of the most probable class.},
	keywords = {Robustness, Sum-Product Networks, Credal Sets, Classification, Fraud Detection},
	section = {Short Papers}
}

@InProceedings{doria19,
	title = {Coherent Upper Conditional Previsions Defined by Hausdorff Outer Measures for  Unbounded Random Variables},
	author = {Doria, Serena},
	pages = {159--166},
	abstract = {A model of upper conditional previsions for bounded and unbounded random variables with finite Choquet integral with respect to the Hausdorff outer and inner measures is proposed in a metric space. They are defined by the Choquet integral with respect to Hausdorff outer measures if the conditioning event has positive and finite Hausdorff outer measure in its dimension, otherwise, when the conditioning event has Hausdorff outer measure equal to zero or infinity in its Hausdorff dimension, they are defined by a 0-1 valued finitely, but not countably, additive probability.},
	keywords = {coherent upper conditional previsions, Hausdorff outer measures, unbounded random variables, Choquet integral, Monotone Convergence Theorem},
	section = {Full Papers}
}

@InProceedings{ebner19,
	title = {Robust Bayes Factor for Independent Two-Sample Comparisons under Imprecise Prior Information},
	author = {Ebner, Luisa and Schwaferts, Patrick Michael and Augustin, Thomas},
	pages = {167--174},
	abstract = {This paper proposes the robust Bayes Factor as a direct generalization of the conventional Bayes Factor for a special case of independent two-sample comparisons. Such comparisons are of great importance in psychological research, and more generally wherever the scientific endeavour is to ascertain a potential group effect. The conventional Bayes Factor as the ratio of the marginal likelihoods under two considered hypotheses demands for a precise, subjective specification of the prior distribution for the parameter of interest. Thus, it lacks the possibility of incorporating prior knowledge that is only available partially. Drawing on the theory of Imprecise Probabilities, the <em>robust</em> Bayes Factor is presented in view of lifting the restrictions on the specification of the prior distribution as being precise. In practice, the robust Bayes Factor approach enables an analyst to specify hyperparameter <em>intervals</em>, whose lengths correspond to the degree of subjective prior uncertainty. Based thereon, a set of (infinitely) many subjective prior distributions is established to substitute one precise prior distribution. Finally, the robust Bayes Factor is defined as an interval, bounded by the minimal and the maximal resultant Bayes Factor values. Latter are obtained by optimizing the conventional Bayes Factor over the predefined set of prior distributions. This explicit incorporation of incomplete prior knowledge increases the feasibility of applying a Bayesian approach to hypothesis comparisons in scientific practice. It reduces error-proneness, enables for an inclusion of multiple perspectives and encourages cautious, more realistic conclusions in hypothesis comparisons.},
	keywords = {Bayes Factor, Imprecise Probabilities, Robustness, Bayesian Statistics, Prior Specification, Psychological Research, Two-Sample Comparison},
	section = {Full Papers}
}

@InProceedings{erreygers19,
	title = {First Steps Towards an Imprecise Poisson Process},
	author = {Erreygers, Alexander and De Bock, Jasper},
	pages = {175--184},
	abstract = {The Poisson process is the most elementary continuous-time stochastic process that models a stream of repeating events. It is uniquely characterised by a single parameter called the rate. Instead of a single value for this rate, we here consider a rate interval and let it characterise two nested sets of stochastic processes. We call these two sets of stochastic process imprecise Poisson processes, explain why this is justified, and study the corresponding lower and upper (conditional) expectations. Besides a general theoretical framework, we also provide practical methods to compute lower and upper (conditional) expectations of functions that depend on the number of events at a single point in time.},
	keywords = {Poisson process, counting process, continuous-time Markov chain, imprecision},
	section = {Full Papers}
}

@InProceedings{fetz19,
	title = {Improving the Convergence of Iterative Importance Sampling for Computing Upper and Lower Expectations},
	author = {Fetz, Thomas},
	pages = {185--193},
	abstract = {The aim of this paper is to present methods for improving the convergence of an iterative importance sampling algorithm for calculating lower and upper expectations with respect to sets of probability distributions. Our focus here is on the reuse and the combination of results obtained in previous iteration steps of the algorithm.},
	keywords = {Monte Carlo simulation, importance sampling, reweighting, imprecise probability, lower/upper expectations, lower/upper probabilities},
	section = {Full Papers}
}

@InProceedings{fink19,
	title = {SIPTA-Community Based on Paper Contributions -- Descriptive Statistics and Network Analysis},
	author = {Fink, Paul},
	pages = {194--202},
	abstract = {The ISIPTA electronic proceedings provide an insight into the SIPTA community. For the anniversary edition of ISIPTA they are analyzed descriptively and in terms of a collaboration network. Different aspects, including paper keywords and geographic location are also investigated. A descriptive analysis of papers reveals how the type of papers changed over the years, including the hot topics by means of an analysis of the keywords of the papers. The network analyses show that there is a core of authors, contributing to ISIPTAs since the beginning, who are now key figures within the collaboration network, attracting new researchers who become key figures themselves.},
	keywords = {collaboration network, author network, ISIPTA paper proceedings, descriptive analysis},
	section = {Full Papers}
}

@InProceedings{fischer19,
	title = {On the Usefulness of Imprecise Bayesianism in Chemical Kinetics},
	author = {Fischer, Marc},
	pages = {203--215},
	abstract = {Bayesian methods are growing ever more popular in chemical kinetics. The reasons for this and general challenges related to kinetic parameter estimation are shortly reviewed. Most authors content themselves with using one single (mostly uniform) prior distribution. The goal of this paper is to go into some serious issues this raises. The problems of confusing knowledge and ignorance and of reparametrisation are examined. The legitimacy of a probabilistic Ockham’s razor is called into question. A synthetic example involving two reaction models was used to illustrate how merging the parameter space volume with the model accuracy into a single number might be unwise. Robust Bayesian analysis appears to be a simple and straightforward way to avoid the problems mentioned throughout this article.},
	keywords = {Imprecise probability, parameter estimation, Bayes' factors, Ockham's razor, Robust Bayesian analysis, chemical kinetics},
	section = {Full Papers}
}

@InProceedings{fuetterer19,
	title = {Constructing Simulation Data with Dependency Structure for Unreliable Single-Cell RNA-Sequencing Data Using Copulas},
	author = {Fuetterer, Cornelia and Schollmeyer, Georg and Augustin, Thomas},
	pages = {216--224},
	abstract = {Simulation studies are becoming increasingly important for the evaluation of complex statistical methods. They tend to represent idealized situations. With our framework, which incorporates dependency structures using copulas, we propose multidimensional simulation data with marginals based on different degrees of heterogeneity, which are built on different ranges of distribution parameters of a zero-inflated negative binomial distribution. The obtained higher and lower variation of the simulation data allows to create lower and upper distribution functions lead to simulation data containing extreme points for each observation. Our approach aims at being closer to reality by considering data distortion. It is an approach of examining classification quality in case of measurement distortions in gene expression data and might propose specific instructions of calibrating measuring instruments.},
	keywords = {Simulation studies, Copula, Imprecise probabilities, Lower and upper distribution function, Distorted measurements, Classification, Single-cell RNA-sequencing data, Statistical genetics},
	section = {Full Papers}
}

@InProceedings{gong19,
	title = {Simultaneous Inference under the Vacuous Orientation Assumption},
	author = {Gong, Ruobin},
	pages = {225--234},
	abstract = {I propose a novel approach to simultaneous inference that alleviates the need to specify a correlational structure among marginal errors. The <em>vacuous orientation</em> assumption retains what the normal i.i.d. assumption implies about the distribution of error configuration, but relaxes the implication that the error orientation is isotropic. When a large number of highly dependent hypotheses are tested simultaneously, the proposed model produces calibrated posterior inference by leveraging the logical relationship among them. This stands in contrast to the conservative performance of the Bonferroni correction, even if neither approaches makes assumptions about error dependence. The proposed model employs the Dempster-Shafer Extended Calculus of Probability, and delivers posterior inference in the form of stochastic three-valued logic.},
	keywords = {Dempster-Shafer theory, belief function, Bonferroni correction, familywise error rate, calibrated inference},
	section = {Full Papers}
}

@InProceedings{hill19,
	title = {Confidence in Belief, Weight of Evidence and Uncertainty Reporting},
	author = {Hill, Brian},
	pages = {235--245},
	abstract = {One way of putting a popular objection to the Bayesian account of belief and decision is that it does not allow a role for something akin to the Keynesian concept of ‘weight of evidence’ in choice. This paper argues that a recently-defended approach, which refines the credal-set representation of beliefs to give pride of place to an agent’s confidence in her beliefs, can do so fruitfully. Motivated by the use of confidence by the IPCC and US Defense Intelligence Agency in their assessments of uncertainty, the paper then considers the consequences of the proposed approach for uncertainty reporting. On the one hand, when connected to decision, the model affords a clear separation of the belief and value factors: an important quality in policy making contexts where these are the responsibility of different actors. On the other hand, the issue of inter-agent confidence calibration is discussed, and a calibration scale is proposed and defended, on the basis of weight-of-evidence version of David Lewis’s Principal Principle.},
	keywords = {confidence in belief, uncertainty reporting, Keynesian weight of evidence, rational decision, confidence ranking, credal sets},
	section = {Full Papers}
}

@InProceedings{keshavarzizafarghandi19,
	title = {Embedding Probabilities, Utilities and Decisions in a Generalization of Abstract Dialectical Frameworks},
	author = {Keshavarzi Zafarghandi, Atefeh and Verheij, Bart and Verbrugge, Rineke},
	pages = {246--255},
	abstract = {Life is made up of a long list of decisions. In each of them there exists quite a number of choices and most of the decisions are effected by uncertainties and preferences, from choosing a healthy lunch and nice clothes to choosing a profession and a field of study. Uncertainties can be modeled by probabilities and preferences by utilities. A rational decision maker prefers to make a decision with the least regret or the most satisfaction. The principle of maximum expected utility can be helpful in this issue. Expected utility deals with problems in which agents make a decision under conditions in which probabilities of states play a role in the choice, as well as the utilities of outcomes. Argumentation formalisms could be an option to model these problems and to pick one or several alternatives. In this paper, a new argument-based framework, numerical abstract dialectical frameworks (nADFs for short), is introduced to do so. First, the semantics of this formalism, which is a generalization of abstract dialectical frameworks (ADFs for short), based on many-valued interpretations are introduced, including preferred, grounded, complete and model-based semantics. Second, it is shown how nADFs are expressive enough to formalize standard decision problems. It is shown that the different types of semantics of an nADF that is associated with a decision problem all coincide and have the standard meaning. In this way, it is shown how the nADF semantics can be used to choose the best set of decisions.},
	keywords = {argumentation, abstract dialectical frameworks, utility, probability, decision problem, expected utility theory},
	section = {Full Papers}
}

@InProceedings{konek19,
	title = {IP Scoring Rules: Foundations and Applications},
	author = {Konek, Jason},
	pages = {256--264},
	abstract = {The mathematical foundations of imprecise probability theory (IP) have been in place for 25 years, and IP has proved successful in practice. But IP methods lack rigorous accuracy-centered, philosophical justifications. Traditional Bayesian methods can be justified using epistemic scoring rules, which measure the accuracy of the estimates that they produce. But there has been little work extending these justifications to the IP framework. This paper makes plea for the IP community to embrace this research programme. The plea comes in three parts. Firstly, I outline some initial work developing scoring rules for imprecise probabilities --- IP scoring rules --- and using them to shore up the philosophical foundations of IP. Secondly, I explain why a range of impossibility results for IP scoring rules should <em>not</em> dissuade the IP community from working on the foundations of IP scoring rules. Finally, I highlight one potential applications for IP scoring rules: IP aggregation.},
	keywords = {IP scoring rules, IP Impossibility Theorems, IP Aggregation},
	section = {Full Papers}
}

@InProceedings{krak19,
	title = {Hitting Times and Probabilities for Imprecise Markov Chains},
	author = {Krak, Thomas and T'Joens, Natan and De Bock, Jasper},
	pages = {265--275},
	abstract = {We consider the problem of characterising expected hitting times and hitting probabilities for imprecise Markov chains. To this end, we consider three distinct ways in which imprecise Markov chains have been defined in the literature: as sets of homogeneous Markov chains, as sets of more general stochastic processes, and as game-theoretic probability models. Our first contribution is that all these different types of imprecise Markov chains have the same lower and upper expected hitting times, and similarly the hitting probabilities are the same for these three types. Moreover, we provide a characterisation of these quantities that directly generalises a similar characterisation for precise, homogeneous Markov chains.},
	keywords = {imprecise Markov chain, hitting time, hitting probability, lower and upper expectations},
	section = {Full Papers}
}

@InProceedings{ma19,
	title = {Making Set-Valued Predictions in Evidential Classification: A Comparison of Different Approaches},
	author = {Ma, Liyao and Denoeux, Thierry},
	pages = {276--285},
	abstract = {In classification, it is often preferable to assign a pattern to a set of classes when the uncertainty is too high to make a precise decision. In this paper, we consider the problem of making set-valued predictions in classification tasks, when uncertainty is described by belief functions. Two approaches are contrasted. In the first one, an act is defined as the assignment to only one class, and we define a partial preorder among acts. The set of non-dominated acts is then given as the prediction. In the second approach, an act is defined as the assignment to a set of classes, and we construct a complete preorder among acts. The two approaches are discussed and compared experimentally. A critical issue both to make decisions and to evaluate decision rules is to define the utility of set-valued prediction. To this end, we propose to model the decision maker’s attitude towards imprecision using an Ordered Weighted Average (OWA) operator, which allows us to extend the utility matrix. An experimental comparison of different decision rules is performed using UCI and artificial Gaussian data sets.},
	keywords = {Belief functions, Dempster-Shafer theory, Decision under uncertainty},
	section = {Full Papers}
}

@InProceedings{martin19a,
	title = {Validity-Preservation Properties of Rules for Combining Inferential Models},
	author = {Martin, Ryan and Syring, Nicholas},
	pages = {286--294},
	abstract = {An inferential model encodes the data analyst’s degrees of belief about an unknown quantity of interest based on the observed data, posited statistical model, etc. Inferences drawn based on these degrees of belief should be reliable in a certain sense, so we require the inferential model to be <em>valid</em>. The construction of valid inferential models based on individual pieces of data is relatively straightforward, but how to combine these so that the validity property is preserved? In this paper we analyze some common combination rules with respect to this question, and we conclude that the best strategy currently available is one that combines via a certain dimension reduction step before the inferential model construction.},
	keywords = {belief function, conditioning, Dempster's rule, Dubois and Prade's rule, plausibility function, random set, statistical inference},
	section = {Full Papers}
}

@InProceedings{martin19b,
	title = {On Valid Uncertainty Quantification About a Model},
	author = {Martin, Ryan},
	pages = {295--303},
	abstract = {Inference on parameters within a given model is familiar, as is ranking different models for the purpose of selection. Less familiar, however, is the quantification of uncertainty about the models themselves. A Bayesian approach provides a posterior distribution for the model but it comes with no validity guarantees, and, therefore, is only suited for ranking and selection. In this paper, I will present an alternative way to view this model uncertainty problem, through the lens of a valid inferential model based on random sets and non-additive beliefs. Specifically, I will show that valid uncertainty quantification about a model is attainable within this framework in general, and highlight the benefits in a classical signal detection problem.},
	keywords = {Bayesian, inferential model, marginalization, plausibility, random set, variable selection},
	section = {Full Papers}
}

@InProceedings{miranda19,
	title = {A Unifying Frame for Neighbourhood and Distortion Models},
	author = {Miranda, Enrique and Montes, Ignacio and Destercke, S\'{e}bastien},
	pages = {304--313},
	abstract = {Neighbourhoods of precise probabilities are instrumental to perform robustness analysis, as they rely on very few parameters. Many such models, sometimes referred to as <em>distortion</em> models, have been proposed in the literature, such as the pari-mutuel model, linear vacuous mixtures or the constant odds ratio model. In this paper, we show that all of them can be represented as probability sets that are neighbourhoods defined over different (pre)-metrics, providing a unified view of such models. We also compare them in terms of a number of properties: precision, number of extreme points, n-monotonicity, … thus providing possible guidelines to pick a neighbourhood rather than another.},
	keywords = {Neighbourhood models, distorted probabilities, pari mutuel model, linear vacuous mixtures, constant odds ratio, total variation distance, Kolmogorov distance},
	section = {Full Papers}
}

@InProceedings{oberguggenberger19,
	title = {Random Set Solutions to Stochastic Wave Equations},
	author = {Oberguggenberger, Michael and Wurzer, Lukas},
	pages = {314--323},
	abstract = {This paper is devoted to three topics. First, to prove a measurability theorem for multifunctions with values in non-metrizable spaces, which is required to show that solutions to stochastic wave equations with interval parameters are random sets; second, to apply the theorem to wave equations in any space dimension; and third, to compute upper and lower probabilities of the values of the solution in the case of one space dimension.},
	keywords = {random sets, fundamental measurability theorem, non-metrizable spaces, stochastic wave equations},
	section = {Full Papers}
}

@InProceedings{pedersen19,
	title = {Dilation and Asymmetric Relevance},
	author = {Pedersen, Arthur Paul and Wheeler, Gregory},
	pages = {324--326},
	abstract = {A characterization result of dilation in terms of positive and negative association admits an extremal counterexample, which we present together with a minor repair of the result. Dilation may be asymmetric whereas covariation itself is symmetric. Dilation is still characterized in terms of positive and negative covariation, however, once the event to be dilated has been specified.},
	keywords = {dilation, sets of probabilities},
	section = {Short Papers}
}

@InProceedings{renooij19,
	title = {On Intercausal Interactions in Probabilistic Relational Models},
	author = {Renooij, Silja and {van der Gaag}, Linda C. and Leray, Philippe},
	pages = {327--329},
	abstract = {Probabilistic relational models (PRMs) extend Bayesian networks beyond propositional expressiveness by allowing the representation of multiple interacting classes. For a specific instance of sets of concrete objects per class, a ground Bayesian network is composed by replicating parts of the PRM. The interactions between the objects that are thereby induced, are not always obvious from the PRM. We demonstrate in this paper that the replicative structure of the ground network in fact constrains the space of possible probability distributions and thereby the possible patterns of intercausal interaction.},
	keywords = {PRM instances, qualitative constraints on probability distributions, intercausal interaction},
	section = {Short Papers}
}

@InProceedings{schollmeyer19,
	title = {A Short Note on the Equivalence of the Ontic and the Epistemic View on Data Imprecision for the Case of Stochastic Dominance for Interval-Valued Data},
	author = {Schollmeyer, Georg},
	pages = {330--337},
	abstract = {In the context of the analysis of interval-valued or set-valued data it is often emphasized that one has to carefully distinguish between an epistemic and an ontic understanding of set-valued data. However, there are cases, for which an ontic and an epistemic view do still lead to exactly the same results of the corresponding data analysis. The present paper is a short note on this fact in the context of the analysis of stochastic dominance for interval-valued data.},
	keywords = {Relational Data Analysis, Stochastic Dominance, Partially Ordered Set, Interval Order, Ontic and Epistemic View, Cautious Data Completion},
	section = {Full Papers}
}

@InProceedings{schwaferts19,
	title = {Imprecise Hypothesis-Based Bayesian Decision Making with Simple Hypotheses},
	author = {Schwaferts, Patrick Michael and Augustin, Thomas},
	pages = {338--345},
	abstract = {Applied real-world decisions are frequently guided by the outcome of hypothesis-based statistical analyses. However, most often relevant information about the phenomenon of interest is available only imprecisely, and misleading results might be obtained, in particular, by either ignoring relevant information or pretending a level of knowledge that is not given. In order to be able to include (partial) information authentically in the imprecise form it is available, this paper tries to extend the framework of hypothesis-based Bayesian decision making with simple hypotheses to be able to deal with imprecise information about the three relevant quantities: hypotheses, prior beliefs, and loss function. Although straightforward at first glance, it appears that by specifying the hypotheses imprecisely, Bayesian updating of the prior beliefs might be inconsistent. In that, this paper provides the basic mathematical formulation to further extend imprecise hypothesis-based Bayesian decision theory to more elaborate contexts, such as those involving composite imprecise hypotheses, and in addition highlights the necessity of paying particular attention to the depicted updating issues.},
	keywords = {Hypotheses, Likelihood Ratio, Imprecise Probabilities, Bayesian Decision Theory, Sequential Updating, Inconsistency, Statistics in Psychological Research},
	section = {Full Papers}
}

@InProceedings{seidenfeld19,
	title = {A Retrospective on Isaac Levi: June 30, 1930 -- December 25, 2018},
	author = {Seidenfeld, Teddy},
	pages = {346--353},
	abstract = {Isaac Levi’s philosophy places him squarely within the tradition of American Pragmatism: the noble legacy of Peirce, James, and Dewey, evidently influenced by his teachers and colleagues at Columbia University, amongst whom E. Nagel and S. Morgenbesser, and fellow graduate students at Columbia University, e.g., H. E. Kyburg, Jr. and F. Schick. Important for understanding Levi’s original perspective on large scale philosophical problems is the theme that decision theory is embedded in them all. Typical of his work, Levi’s contributions are grounded on significant distinctions, many of which are cast with the aid of sound decision-theory. In this retrospective I review four salient examples of his interests, spanning Levi’s work on 1) belief acceptance, 2) belief revision, 3) social philosophy, and 4) statistical inference.},
	keywords = {pragmatism, belief acceptance, belief revision, social agents},
	section = {Full Papers}
}

@InProceedings{skulj19,
	title = {Extensions of Sets of Markov Operators Under Epistemic Irrelevance},
	author = {Skulj, Damjan},
	pages = {354--363},
	abstract = {Sets of Markov operators can serve as generalised models for imprecise probabilities. They act on gambles as transformations preserving desirability. Often imprecise probabilistic models and also sets of operators need to be extended to larger domains. Such extensions are especially interesting when some kind of independence requirements have to be taken into account. The goal of this paper is to propose extension methods for sets of Markov operators that are consistent with the existing extension methods for imprecise probabilistic models. The main focus is on extensions satisfying epistemic irrelevance. We propose a new general approach to extending sets of desirable gambles, called <em>additive independent extension</em>, which subsumes important types of extensions, such as epistemic irrelevance and marginal extension. This approach is then extended to sets of Markov operators, so that the extensions are consistent with those of sets of desirable gambles.},
	keywords = {epistemic irrelevance, Markov operator, desirable gamble, additive independent extension},
	section = {Full Papers}
}

@InProceedings{smithson19a,
	title = {Imprecise Compositional Data Analysis: Alternative Statistical Methods},
	author = {Smithson, Michael},
	pages = {364--366},
	abstract = {This paper briefly describes statistical methods for analyzing imprecise compositional data that might be elicited from approximate measurement or from expert judgments. Two alternative approaches are discussed: Log-ratio transforms and probability-ratio transforms. The first is well-established and the second is under development by the author. The primary focus in this paper is on generalized linear models for predicting imprecise compositional data.},
	keywords = {compositional data, imprecise data, beta distribution, cdf-quantile distribution, general linear model, copula},
	section = {Short Papers}
}

@InProceedings{smithson19b,
	title = {Incompletely Known Sample Spaces: Models and Human Intuitions},
	author = {Smithson, Michael},
	pages = {367--376},
	abstract = {This paper surveys models and human intuitions about incompletely known “sample spaces” ($\Omega$). Given that there are very few guidelines for how best to form such beliefs when $\Omega$ is incompletely known, and there is very little research on the psychology behind beliefs about $\Omega$, this survey is preliminary and brings in ideas and models from probability and statistics, biology, and psychology. Pilot experimental studies of how people estimate the cardinality of $\Omega$ when given sample information from it are presented, demonstrating that to a surprising extent their estimates correspond with those produced by normative statistical models. The paper concludes by outlining future directions for a research program on this topic.},
	keywords = {sample space, cardinality, capture-recapture sample, Dirichlet process, imprecise Dirichlet model, human intuition},
	section = {Full Papers}
}

@InProceedings{tjoens19,
	title = {In Search of a Global Belief Model for Discrete-Time Uncertain Processes},
	author = {T'Joens, Natan and De Bock, Jasper and {de Cooman}, Gert},
	pages = {377--385},
	abstract = {To model discrete-time uncertain processes, we argue for the use of a global belief model in the form of an upper expectation that satisfies a number of simple and intuitive axioms. We motivate these axioms on the basis of two possible interpretations for this upper expectation: a behavioural interpretation similar to that of Walley’s, and an interpretation in terms of upper envelopes of linear expectations. Subsequently, we show that the most conservative upper expectation satisfying our axioms coincides with a particular version of the game-theoretic upper expectation introduced by Shafer and Vovk. This has two important implications. On the one hand, it guarantees that there is a unique most conservative global belief model satisfying our axioms. On the other hand, it shows that Shafer and Vovk’s model can be given an axiomatic characterisation, thereby providing an alternative motivation for adopting this model, even outside their framework.},
	keywords = {Game-theoretic probability, Upper expectations, Uncertain processes, Coherence},
	section = {Full Papers}
}

@InProceedings{troffaes19a,
	title = {A Cantelli-Type Inequality for Constructing Non-Parametric P-Boxes Based on Exchangeability},
	author = {Troffaes, Matthias and Basu, Tathagata},
	pages = {386--393},
	abstract = {In this paper we prove a new probability inequality that can be used to construct p-boxes in a non-parametric fashion, using the sample mean and sample standard deviation instead of the true mean and true standard deviation. The inequality relies only on exchangeability and boundedness.},
	keywords = {probability inequality, p-box, exchangeability, Cantelli, Chebyshev},
	section = {Full Papers}
}

@InProceedings{troffaes19b,
	title = {Two-State Imprecise Markov Chains for Statistical Modelling of Two-State Non-Markovian Processes},
	author = {Troffaes, Matthias and Krak, Thomas and Bains, Henna},
	pages = {394--403},
	abstract = {This paper proposes a method for fitting a two-state imprecise Markov chain to time series data from a two-state non-Markovian process. Such non-Markovian processes are common in practical applications. We focus on how to fit modelling parameters based on data from a process where time to transition is not exponentially distributed, thereby violating the Markov assumption. We do so by first fitting a many-state (i.e. having more than two states) Markov chain to the data, through its associated phase-type distribution. Then, we lump the process to a two-state imprecise Markov chain. In practical applications, a two-state imprecise Markov chain might be more convenient than a many-state Markov chain, as we have closed analytic expressions for typical quantities of interest (including the lower and upper expectation of any function of the state at any point in time). A numerical example demonstrates how the entire inference process (fitting and prediction) can be done using Markov chain Monte Carlo, for a given set of prior distributions on the parameters. In particular, we numerically identify the set of posterior densities and posterior lower and upper expectations on all model parameters and predictive quantities. We compare our inferences under a range of sample sizes and model assumptions.},
	keywords = {imprecise Markov chain, estimation, reliability, Markov assumption, MCMC},
	section = {Full Papers}
}

@InProceedings{utkin19,
	title = {Imprecise Extensions of Random Forests and Random Survival Forests},
	author = {Utkin, Lev and Kovalev, Maxim and Meldo, Anna and Coolen, Frank},
	pages = {404--413},
	abstract = {Robust weighted aggregation schemes taking into account imprecision of the decision tree estimates in random forests and in random survival forests are proposed in the paper. The first scheme dealing with the random forest improves the classification problem solution. The second scheme dealing with the random survival forest improves the survival analysis task solution. The main idea underlying the proposed modifications is to introduce the tree weights which take simultaneously into account imprecision of estimations as well as aims of the classification and regression problems. The imprecision of the tree estimates is defined by means of imprecise statistical inference models and interval models. Special modifications of loss functions for the classification and regression tasks are proposed in order to simplify minimax and maximin optimization problems for computing optimal weights. Numerical examples illustrate the proposed robust models.},
	keywords = {classification, survival analysis, random forest, decision tree, deep forest, imprecise Dirichlet model, imprecise probabilities},
	section = {Full Papers}
}

@InProceedings{vancamp19,
	title = {Irrelevant Natural Extension for Choice Functions},
	author = {Van Camp, Arthur and Miranda, Enrique},
	pages = {414--423},
	abstract = {We consider coherent choice functions under the recent axiomatisation proposed by De Bock and De Cooman that guarantees a representation in terms of binary preferences, and we discuss how to define conditioning in this framework. In a multivariate context, we propose a notion of marginalisation, and its inverse operation called weak (cylindrical) extension. We combine this with our definition of conditioning to define a notion of irrelevance, and we obtain the irrelevant natural extension in this framework: the least informative choice function that satisfies a given irrelevance assessment.},
	keywords = {Choice functions, coherence, sets of desirable gambles, natural extension, conditioning, epistemic irrelevance},
	section = {Full Papers}
}

@InProceedings{vanommen19,
	title = {Robust Causal Domain Adaptation in a Simple Diagnostic Setting},
	author = {{van Ommen}, Thijs},
	pages = {424--429},
	abstract = {Causal domain adaptation approaches aim to find statistical relations in a source domain, that will still hold in a target domain, using the assumption that a common causal graph underlies both domains. For many such problems, the available information is insufficient to uniquely identify the target domain distribution, and we find a set of distributions instead. We propose to use a worst-case approach, picking an action that performs well against all distributions in this set. In this paper, we study a specific diagnostic instance of this problem, and find a sufficient and necessary condition that characterizes the worst-case distribution in the target domain. We find that the Brier and logarithmic scores lead to different distributions, and consequently to different recommendations for the decision maker.},
	keywords = {domain adaptation, causal graph, minimax decision making, robust Bayes, scoring function},
	section = {Full Papers}
}

@InProceedings{villanuevallerena19,
	title = {Robust Analysis of MAP Inference in Selective Sum-Product Networks},
	author = {Villanueva Llerena, Julissa Giuliana and Mau\'{a}, Denis Deratani},
	pages = {430--440},
	abstract = {Sum-Product Networks (SPN) are deep probabilistic models that have shown to achieve state-of-the-art performance in several machine learning tasks. As with many other probabilistic models, performing Maximum-A-Posteriori (MAP) inference is NP-hard in SPNs. A notable exception is selective SPNs, that constrain the network so as to allow MAP inference to be performed in linear time. Due to the high number of parameters, SPNs learned from data can produce unreliable and overconfident inference; this phenomenon can be partially mitigated by performing a Robustness Analysis of the model predictions to changes in the parameters. In this work, we address the problem of assessing the robustness of MAP inferences produced with Selective SPNs to global perturbations of the parameters. We present efficient algorithms and an empirical analysis with realistic problems.},
	keywords = {Robust statistics, sensitivity analysis, sum-product networks, tractable probabilistic models},
	section = {Full Papers}
}

